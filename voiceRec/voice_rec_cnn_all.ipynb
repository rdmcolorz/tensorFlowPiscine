{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backend_bases import RendererBase\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "#import soundfile as sf\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.fftpack import fft\n",
    "from scipy.misc import imresize\n",
    "from IPython.core.display import HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "import operator\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import random\n",
    "import tensorflow_hub as hub\n",
    "%matplotlib inline\n",
    "# tf.logging.set_verbosity(tf.logging.INFO)\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "audio_path = './input/train/audio/'\n",
    "pict_Path = './input/picts/newtrain/'\n",
    "test_pict_Path = './input/picts/test/'\n",
    "test_audio_path = './input/test/audio/'\n",
    "samples = []\n",
    "\n",
    "# os.system('rm -rf ./input/picts/test')\n",
    "# os.system('rm -rf ./input/picts/train')\n",
    "# os.mkdir('./input/picts/test')\n",
    "# os.mkdir('./input/picts/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subFolderList = []\n",
    "for x in os.listdir(audio_path):\n",
    "    if os.path.isdir(audio_path + '/' + x):\n",
    "        subFolderList.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: './input/picts/newtrain/yes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0ff387249ee2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels_to_keep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpict_Path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# for f in labels_to_keep:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: './input/picts/newtrain/yes'"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(pict_Path):\n",
    "    os.makedirs(pict_Path)\n",
    "\n",
    "if not os.path.exists(test_pict_Path):\n",
    "    os.makedirs(test_pict_Path)\n",
    "\n",
    "\n",
    "labels_to_keep = ['yes', 'no', 'up', 'down', 'left',\n",
    "                  'right', 'on', 'off', 'stop', 'go', 'silence', 'unknown']\n",
    "\n",
    "for f in labels_to_keep:\n",
    "    os.mkdir(pict_Path + f)\n",
    "    \n",
    "# for f in labels_to_keep:\n",
    "#     os.mkdir(test_pict_Path + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 1733 : tree\n",
      "count: 2357 : five\n",
      "count: 1746 : dog\n",
      "count: 1742 : happy\n",
      "count: 2372 : go\n",
      "count: 2367 : right\n",
      "count: 395 : _background_noise_\n",
      "count: 1746 : marvin\n",
      "count: 2375 : no\n",
      "count: 2352 : eight\n",
      "count: 2370 : one\n",
      "count: 1713 : bed\n",
      "count: 1731 : bird\n",
      "count: 2364 : nine\n",
      "count: 1745 : wow\n",
      "count: 2373 : two\n",
      "count: 2367 : on\n",
      "count: 2380 : stop\n",
      "count: 1750 : house\n",
      "count: 2353 : left\n",
      "count: 2377 : seven\n",
      "count: 2356 : three\n",
      "count: 1734 : sheila\n",
      "count: 2376 : zero\n",
      "count: 1733 : cat\n",
      "count: 2372 : four\n",
      "count: 2357 : off\n",
      "count: 2377 : yes\n",
      "count: 2359 : down\n",
      "count: 2375 : up\n",
      "count: 2369 : six\n",
      "65116\n"
     ]
    }
   ],
   "source": [
    "sample_audio = []\n",
    "total = 0\n",
    "for x in subFolderList:\n",
    "    \n",
    "    # get all the wave files\n",
    "    all_files = [y for y in os.listdir(audio_path + x) if '.wav' in y]\n",
    "    total += len(all_files)\n",
    "    # collect the first file from each dir\n",
    "    sample_audio.append(audio_path  + x + '/'+ all_files[0])\n",
    "    \n",
    "    # show file counts\n",
    "    print('count: %d : %s' % (len(all_files), x ))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_specgram(audio, sample_rate, window_size=20,\n",
    "                 step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, _, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, np.log(spec.T.astype(np.float32) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wav2img(wav_path, targetdir='', figsize=(4,4)):\n",
    "    samplerate, test_sound  = wavfile.read(wav_path)\n",
    "    _, spectrogram = log_specgram(test_sound, samplerate)\n",
    "    spectrogram = imresize(spectrogram, (28,28))\n",
    "    \n",
    "    output_file = wav_path.split('/')[-1].split('.wav')[0]\n",
    "    output_file = targetdir +'/'+ output_file\n",
    "    #plt.imshow(spectrogram.T, aspect='auto', origin='lower')\n",
    "    plt.imsave('%s.jpg' % output_file, spectrogram)\n",
    "    plt.close()\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def wav2img(wav_path, targetdir='', figsize=(4,4)):\n",
    "#     \"\"\"\n",
    "#     takes in wave file path\n",
    "#     and the fig size. Default 4,4 will make images 288 x 288\n",
    "#     \"\"\"\n",
    "# #     fig = plt.figure(figsize=figsize)\n",
    "#     # use soundfile library to read in the wave files\n",
    "#     samplerate, test_sound  = wavfile.read(wav_path)\n",
    "#     _, spectrogram = log_specgram(test_sound, samplerate)\n",
    "    \n",
    "# #     space = spectrogram.shape[0]\n",
    "# #     if space < 161:\n",
    "# #         spectrogram = np.pad(spectrogram, ((161-space,0), (0,0)), mode='constant', constant_values=0)\n",
    "#     spectrogram = imresize(spectrogram, (96,96))\n",
    "# #     spectrogram = tf.reshape(spectrogram, [-1, 161, 161, 1])\n",
    "# #     spectrogram = tf.image.resize_images(spectrogram, (96, 96))\n",
    "# #     spectrogram = tf.image.grayscale_to_rgb(spectrogram)\n",
    "    \n",
    "#     ## create output path\n",
    "#     output_file = wav_path.split('/')[-1].split('.wav')[0]\n",
    "#     output_file = targetdir +'/'+ output_file\n",
    "#     #plt.imshow(spectrogram.T, aspect='auto', origin='lower')\n",
    "#     plt.imsave('%s.jpg' % output_file, spectrogram)\n",
    "#     plt.close()\n",
    "#     return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wav2img_waveform(wav_path, targetdir='', figsize=(4,4)):\n",
    "    samplerate,test_sound  = wavfile.read(sample_audio[0])\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    plt.plot(test_sound)\n",
    "    plt.axis('off')\n",
    "    output_file = wav_path.split('/')[-1].split('.wav')[0]\n",
    "    output_file = targetdir +'/'+ output_file\n",
    "    plt.savefig('%s.png' % output_file)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_to_keep = ['yes', 'no', 'up', 'down', 'left',\n",
    "                  'right', 'on', 'off', 'stop', 'go']\n",
    "\n",
    "test_labels = ['yes', 'no', 'up', 'down', 'left',\n",
    "                  'right', 'on', 'off', 'stop', 'go', 'silence', 'unknown']\n",
    "\n",
    "label_dict = {'yes':0, 'no':1, 'up':2, 'down':3, 'left':4,\n",
    "                  'right':5, 'on':6, 'off':7, 'stop':8, 'go':9, 'silence':10, 'unknown':11}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i, x in enumerate(['yes', 'no']):\n",
    "#         all_files = [y for y in os.listdir(audio_path + x) if '.wav' in y]\n",
    "#         for file in all_files[:]:\n",
    "#             wav2img(audio_path + x + '/' + file, './input/yes_no/' + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : tree\n",
      "1 : five\n",
      "2 : dog\n",
      "3 : happy\n",
      "4 : go\n",
      "5 : right\n",
      "6 : _background_noise_\n",
      "7 : marvin\n",
      "8 : no\n",
      "9 : eight\n",
      "10 : one\n",
      "11 : bed\n",
      "12 : bird\n",
      "13 : nine\n",
      "14 : wow\n",
      "15 : two\n",
      "16 : on\n",
      "17 : stop\n",
      "18 : house\n",
      "19 : left\n",
      "20 : seven\n",
      "21 : three\n",
      "22 : sheila\n",
      "23 : zero\n",
      "24 : cat\n",
      "25 : four\n",
      "26 : off\n",
      "27 : yes\n",
      "28 : down\n",
      "29 : up\n",
      "30 : six\n"
     ]
    }
   ],
   "source": [
    "#code for turning wav files into jpg.\n",
    "for i, x in enumerate(subFolderList):\n",
    "    if x in labels_to_keep:\n",
    "        print(i, ':', x)\n",
    "        # get all the wave files\n",
    "        all_files = [y for y in os.listdir(audio_path + x) if '.wav' in y]\n",
    "        for file in all_files[:]:\n",
    "            wav2img(audio_path + x + '/' + file, pict_Path + x)\n",
    "    elif x == '_background_noise_':\n",
    "        print(i, ':', x)\n",
    "        # get all the wave files\n",
    "        all_files = [y for y in os.listdir(audio_path + x) if '.wav' in y]\n",
    "        for file in all_files[:]:\n",
    "            wav2img(audio_path + x + '/' + file, \"./input/picts/train/silence\")\n",
    "    else:\n",
    "        print(i, ':', x)\n",
    "        all_files = [y for y in os.listdir(audio_path + x) if '.wav' in y]\n",
    "        for file in all_files[:]:\n",
    "            wav2img(audio_path + x + '/' + file, \"./input/picts/train/unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i, x in enumerate(subFolderList):\n",
    "#     if x in labels_to_keep:\n",
    "#         print(i, ':', x)\n",
    "#         # get all the wave files\n",
    "#         all_files = [y for y in os.listdir(audio_path + x) if '.wav' in y]\n",
    "#         for file in all_files[101:120]:\n",
    "#             wav2img(audio_path + x + '/' + file, test_pict_Path + x)\n",
    "#     else:\n",
    "#         all_files = [y for y in os.listdir(audio_path + x) if '.wav' in y]\n",
    "#         for file in all_files[16:20]:\n",
    "#             wav2img(audio_path + x + '/' + file, \"./input/picts/test/unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test = tf.image.decode_png(\"./input/picts/train/down/00b01445_nohash_1.png\",3)\n",
    "# sess = tf.Session()\n",
    "# print(sess.run(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23919, 96, 96, 3)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "train_images = []\n",
    "train_labels = []\n",
    "for i, x in enumerate(test_labels): #choose the classes to train\n",
    "    all_files = [y for y in os.listdir(pict_Path + x) if '.jpg' in y]\n",
    "    if x == \"unknown\":\n",
    "        random.shuffle(all_files)\n",
    "    for file in all_files[:1995]:\n",
    "        train_images.append(cv2.imread(pict_Path + x + '/' + file))\n",
    "        train_labels.append(label_dict[x])\n",
    "\n",
    "train_images = np.array(train_images, dtype=\"float32\")\n",
    "train_labels = np.array(train_labels)\n",
    "print(train_images.shape)\n",
    "print(type(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23919,)\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0 ..., 11 11 11]\n",
      "[ 1  9 10 ...,  8 10  4]\n"
     ]
    }
   ],
   "source": [
    "# code to shuffe feature and label arrays.\n",
    "train_labels.reshape((1, -1))\n",
    "train_labels = np.array(train_labels, dtype=\"int32\")\n",
    "randomize = np.arange(len(train_labels))\n",
    "np.random.shuffle(randomize)\n",
    "print(train_labels)\n",
    "labels = train_labels[randomize]\n",
    "features = train_images[randomize]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23919\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  9 10 ...,  3  7  0]\n",
      "[ 4  8  1 ...,  8 10  4]\n"
     ]
    }
   ],
   "source": [
    "# code to split training and validation data\n",
    "x = len(labels)\n",
    "x *= 0.8\n",
    "x = int(x)\n",
    "training_examples = features[:x]\n",
    "validation_examples = features[x:]\n",
    "\n",
    "training_targets = labels[:x]\n",
    "validation_targets = labels[x:]\n",
    "print(training_targets)\n",
    "print(validation_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mobilenet_model_fn(features, labels, mode):\n",
    "    # Load Inception-v3 model.\n",
    "    \n",
    "    module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_035_96/feature_vector/2\")\n",
    "    input_layer = features\n",
    "    outputs = module(input_layer)\n",
    "    logits = tf.layers.dense(inputs=outputs, units=12) #\n",
    "    \n",
    "    predictions = {\n",
    "        # Generate predictions (for PREDICT and EVAL mode)\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "        # `logging_hook`.\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.005)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "            labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_feature_columns():\n",
    "  # There are 9216 pixels in each image.\n",
    "  return set([tf.feature_column.numeric_column('pixels', shape=9216)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_predict_input_fn(features, labels, batch_size):#, num_epochs=None, shuffle=True):\n",
    "#     print(\"predict input fn\")\n",
    "#     print(type(features))\n",
    "#     print(type(labels))\n",
    "#     print(type(batch_size))\n",
    "    predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x=features,\n",
    "        y=np.array(labels),\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "    return predict_input_fn\n",
    "\n",
    "def create_training_input_fn(features, labels, batch_size):#, num_epochs=None, shuffle=True):\n",
    "#     print(\"training input fn\")\n",
    "#     print(type(features))\n",
    "#     print(type(labels))\n",
    "#     print(type(batch_size))\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x=features,\n",
    "        y=np.array(labels),\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=None,\n",
    "        shuffle=True)\n",
    "    return train_input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_hub_classification_model(\n",
    "    learning_rate,\n",
    "    steps,\n",
    "    batch_size,\n",
    "    hidden_units,\n",
    "    training_examples,\n",
    "    training_targets,\n",
    "    validation_examples,\n",
    "    validation_targets):\n",
    "\n",
    "    periods = 10\n",
    "    steps_per_period = steps / periods \n",
    "#     tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "# #     logging_hook = tf.train.LoggingTensorHook(\n",
    "# #         tensors=tensors_to_log, every_n_iter=10)\n",
    "    \n",
    "    predict_training_input_fn = create_predict_input_fn(training_examples, training_targets, batch_size)\n",
    "    predict_validation_input_fn = create_predict_input_fn(validation_examples, validation_targets, batch_size)\n",
    "  \n",
    "    training_input_fn = create_training_input_fn(training_examples, training_targets, batch_size)\n",
    "    predict_training_input_fn = create_predict_input_fn(training_examples, training_targets, batch_size)\n",
    "    \n",
    "    predict_validation_input_fn = create_predict_input_fn(validation_examples, validation_targets, batch_size)\n",
    "    training_input_fn = create_training_input_fn(training_examples, training_targets, batch_size)\n",
    "  \n",
    "    feature_columns = [tf.feature_column.numeric_column('pixels', shape=9216)]\n",
    "\n",
    "    my_optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "###############################################################\n",
    "    classifier = tf.estimator.Estimator(\n",
    "    model_fn=mobilenet_model_fn,\n",
    "    model_dir=\"./all_model\"\n",
    "    )\n",
    "##############################################################################\n",
    "    # Train the model, but do so inside a loop so that we can periodically assess\n",
    "    # loss metrics.\n",
    "    print(\"Training model...\")\n",
    "    print(\"LogLoss error (on validation data):\")\n",
    "    training_errors = []\n",
    "    validation_errors = []\n",
    "    for period in range (0, periods):\n",
    "        classifier.train(\n",
    "            input_fn=training_input_fn,\n",
    "            steps=steps_per_period,\n",
    "        )\n",
    "        training_predictions = list(classifier.predict(input_fn=predict_training_input_fn))\n",
    "        training_probabilities = np.array([item['probabilities'] for item in training_predictions])\n",
    "        training_pred_class_id = np.array([item['classes'] for item in training_predictions])\n",
    "        training_pred_one_hot = tf.keras.utils.to_categorical(training_pred_class_id,11) #change class number\n",
    "\n",
    "        validation_predictions = list(classifier.predict(input_fn=predict_validation_input_fn))\n",
    "        validation_probabilities = np.array([item['probabilities'] for item in validation_predictions])    \n",
    "        validation_pred_class_id = np.array([item['classes'] for item in validation_predictions])\n",
    "        validation_pred_one_hot = tf.keras.utils.to_categorical(validation_pred_class_id,11) #change class number\n",
    "\n",
    "        # Compute training and validation errors.\n",
    "        training_log_loss = metrics.log_loss(training_targets, training_pred_one_hot)\n",
    "        validation_log_loss = metrics.log_loss(validation_targets, validation_pred_one_hot)\n",
    "        # Occasionally print the current loss.\n",
    "        print(\"  period %02d : %0.2f\" % (period, validation_log_loss))\n",
    "        # Add the loss metrics from this period to our list.\n",
    "        training_errors.append(training_log_loss)\n",
    "        validation_errors.append(validation_log_loss)\n",
    "        \n",
    "    print(\"Model training finished.\")\n",
    "    # Remove event files to save disk space.\n",
    "    _ = map(os.remove, glob.glob(os.path.join(classifier.model_dir, 'events.out.tfevents*')))\n",
    "\n",
    "    # Calculate final predictions (not probabilities, as above).\n",
    "    final_predictions = classifier.predict(input_fn=predict_validation_input_fn)\n",
    "    final_predictions = np.array([item['classes'] for item in final_predictions])\n",
    "\n",
    "\n",
    "    accuracy = metrics.accuracy_score(validation_targets, final_predictions)\n",
    "    print(\"Final accuracy (on validation data): %0.2f\" % accuracy)\n",
    "\n",
    "    # Output a graph of loss metrics over periods.\n",
    "    plt.ylabel(\"LogLoss\")\n",
    "    plt.xlabel(\"Periods\")\n",
    "    plt.title(\"LogLoss vs. Periods\")\n",
    "    plt.plot(training_errors, label=\"training\")\n",
    "    plt.plot(validation_errors, label=\"validation\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Output a plot of the confusion matrix.\n",
    "    cm = metrics.confusion_matrix(validation_targets, final_predictions)\n",
    "    # Normalize the confusion matrix by row (i.e by the number of samples\n",
    "    # in each class).\n",
    "    cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "    ax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\n",
    "    ax.set_aspect(1)\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.show()\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_targets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0f03d9069830>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#validation_targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_targets' is not defined"
     ]
    }
   ],
   "source": [
    "len(set(training_targets))\n",
    "#validation_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_examples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-18da81bcc3f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mhidden_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtraining_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtraining_targets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvalidation_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_examples' is not defined"
     ]
    }
   ],
   "source": [
    "# training for 5000 steps, got 20% accuracy.\n",
    "classifier = train_hub_classification_model(\n",
    "    learning_rate=0.1,\n",
    "    steps=100,\n",
    "    batch_size=100,\n",
    "    hidden_units=[200, 200],\n",
    "    training_examples=training_examples,\n",
    "    training_targets=training_targets,\n",
    "    validation_examples=validation_examples,\n",
    "    validation_targets=validation_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !sudo shutdown -h now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def my_test_input_fn(features, batch_size=1, shuffle=True, num_epochs=None):\n",
    "#     raw_features = {\"pixels\": features.values}\n",
    "#     ds = Dataset.from_tensor_slices((raw_features))\n",
    "#     ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "#     if shuffle:\n",
    "#         ds = ds.shuffle(10000)\n",
    "#     features = ds.make_one_shot_iterator().get_next()\n",
    "#     return features\n",
    "\n",
    "# predict_test_input_fn = lambda: my_test_input_fn(\n",
    "#     test,\n",
    "#     num_epochs=1,\n",
    "#     shuffle=False)\n",
    "\n",
    "# test_predictions = classifier.predict(input_fn=predict_test_input_fn)\n",
    "# test_predictions = np.array([item['classes'] for item in test_predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# my_submission = pd.DataFrame({'fname': test.index + 1, 'label': test_predictions})\n",
    "# # you could use any filename. We choose submission here\n",
    "# my_submission.to_csv('sub_voice_transfer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
