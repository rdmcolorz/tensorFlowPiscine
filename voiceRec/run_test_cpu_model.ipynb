{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backend_bases import RendererBase\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "#import soundfile as sf\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.fftpack import fft\n",
    "from scipy.misc import imresize\n",
    "from IPython.display import Image\n",
    "import mobilenet_model as mobilenet_v1\n",
    "from IPython.core.display import HTML\n",
    "from tensorflow.contrib.framework.python.ops import arg_scope\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from absl import flags\n",
    "import tensorflow as tf\n",
    "import operator\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import random\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.python.data import Dataset\n",
    "%matplotlib inline\n",
    "# tf.logging.set_verbosity(tf.logging.INFO)\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pict_path = './input/picts/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'yes',\n",
       " 1: 'no',\n",
       " 2: 'up',\n",
       " 3: 'down',\n",
       " 4: 'left',\n",
       " 5: 'right',\n",
       " 6: 'on',\n",
       " 7: 'off',\n",
       " 8: 'stop',\n",
       " 9: 'go',\n",
       " 10: 'silence',\n",
       " 11: 'unknown'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_keep = ['yes', 'no', 'up', 'down', 'left',\n",
    "                  'right', 'on', 'off', 'stop', 'go']\n",
    "\n",
    "test_labels = ['yes', 'no', 'up', 'down', 'left',\n",
    "                  'right', 'on', 'off', 'stop', 'go', 'silence', 'unknown']\n",
    "\n",
    "label_dict = {'yes':0, 'no':1, 'up':2, 'down':3, 'left':4,\n",
    "                  'right':5, 'on':6, 'off':7, 'stop':8, 'go':9, 'silence':10, 'unknown':11}\n",
    "\n",
    "inverted_dict = dict([[v,k] for k,v in label_dict.items()])\n",
    "inverted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-00d44a5c708a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtest_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pict_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mfnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtest_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all_array_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all_labels_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_files = [y for y in os.listdir(test_pict_path) if '.jpg' in y]\n",
    "for x in range(0,15):\n",
    "    test_images = []\n",
    "    fnames = []\n",
    "    x *= 10000\n",
    "    for file in all_files[:x]:\n",
    "        test_images.append(cv2.imread(test_pict_path + file))\n",
    "        fnames.append(file[:-4] + '.wav')\n",
    "    test_images = np.array(test_images, dtype=\"float32\")\n",
    "    np.save('all_array_' + str(x), test_images)\n",
    "    np.save('all_labels_' + str(x), fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mobilenet_model_fn(features, labels, mode):\n",
    "    # Load Inception-v3 model.\n",
    "    \n",
    "    module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_035_96/feature_vector/2\")\n",
    "    input_layer = features\n",
    "    outputs = module(input_layer)\n",
    "    logits = tf.layers.dense(inputs=outputs, units=12) #\n",
    "    \n",
    "    predictions = {\n",
    "        # Generate predictions (for PREDICT and EVAL mode)\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "        # `logging_hook`.\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "            labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = tf.estimator.Estimator(\n",
    "    model_fn=mobilenet_model_fn,\n",
    "    model_dir=\"./all_model\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['right', 'off', 'up', 'off', 'on', 'up', 'down', 'off', 'no', 'on', 'silence', 'silence', 'left', 'silence', 'no', 'right', 'down', 'on', 'on', 'on', 'stop', 'down', 'on', 'on', 'down', 'off', 'stop', 'stop', 'right', 'silence', 'silence', 'left', 'down', 'stop', 'silence', 'stop', 'off', 'down', 'on', 'off']\n"
     ]
    }
   ],
   "source": [
    "def my_test_input_fn(features, batch_size=1, shuffle=True, num_epochs=None):\n",
    "    raw_features = features\n",
    "    ds = Dataset.from_tensor_slices((raw_features))\n",
    "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(10000)\n",
    "    features = ds.make_one_shot_iterator().get_next()\n",
    "    return features\n",
    "\n",
    "predict_test_input_fn = lambda: my_test_input_fn(\n",
    "    test_images,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "\n",
    "test_predictions = classifier.predict(input_fn=predict_test_input_fn)\n",
    "test_predictions = np.array([item['classes'] for item in test_predictions])\n",
    "test_predictions = list(map(inverted_dict.get, test_predictions))\n",
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_submission = pd.DataFrame({'fname': fnames, 'label': test_predictions})\n",
    "# you could use any filename. We choose submission here\n",
    "my_submission.to_csv('sub_voice_tpu.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
